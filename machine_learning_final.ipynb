{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1e5a91b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 42 subjects; label counts = [23 19]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s.dharia-ra/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/s.dharia-ra/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/s.dharia-ra/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/s.dharia-ra/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/s.dharia-ra/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 92\u001b[0m\n\u001b[1;32m     90\u001b[0m X_tr_win \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack([windows[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tr_idx])\n\u001b[1;32m     91\u001b[0m km \u001b[38;5;241m=\u001b[39m KMeans(n_clusters\u001b[38;5;241m=\u001b[39mk, random_state\u001b[38;5;241m=\u001b[39mseed, n_init\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 92\u001b[0m \u001b[43mkm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr_win\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m dwell_tr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack([dwell_times(km\u001b[38;5;241m.\u001b[39mpredict(windows[i]), k) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tr_idx])\n\u001b[1;32m     95\u001b[0m dwell_te \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack([dwell_times(km\u001b[38;5;241m.\u001b[39mpredict(windows[i]), k) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m te_idx])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1510\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1507\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitialization complete\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;66;03m# run a k-means once\u001b[39;00m\n\u001b[0;32m-> 1510\u001b[0m labels, inertia, centers, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[43mkmeans_single\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1512\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1513\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcenters_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1514\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1515\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_n_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1518\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1520\u001b[0m \u001b[38;5;66;03m# determine if these results are the best so far\u001b[39;00m\n\u001b[1;32m   1521\u001b[0m \u001b[38;5;66;03m# we chose a new run if it has a better inertia and the clustering is\u001b[39;00m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# different from the best so far (it's possible that the inertia is\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# slightly better even if the clustering is the same with potentially\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;66;03m# permuted labels, due to rounding errors)\u001b[39;00m\n\u001b[1;32m   1525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m best_inertia \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   1526\u001b[0m     inertia \u001b[38;5;241m<\u001b[39m best_inertia\n\u001b[1;32m   1527\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_same_clustering(labels, best_labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_clusters)\n\u001b[1;32m   1528\u001b[0m ):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/parallel.py:165\u001b[0m, in \u001b[0;36m_threadpool_controller_decorator.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m controller \u001b[38;5;241m=\u001b[39m _get_threadpool_controller()\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m controller\u001b[38;5;241m.\u001b[39mlimit(limits\u001b[38;5;241m=\u001b[39mlimits, user_api\u001b[38;5;241m=\u001b[39muser_api):\n\u001b[0;32m--> 165\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:700\u001b[0m, in \u001b[0;36m_kmeans_single_lloyd\u001b[0;34m(X, sample_weight, centers_init, max_iter, verbose, tol, n_threads)\u001b[0m\n\u001b[1;32m    697\u001b[0m strict_convergence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iter):\n\u001b[0;32m--> 700\u001b[0m     \u001b[43mlloyd_iter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcenters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcenters_new\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_in_clusters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcenter_shift\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m    712\u001b[0m         inertia \u001b[38;5;241m=\u001b[39m _inertia(X, sample_weight, centers, labels, n_threads)\n",
      "File \u001b[0;32m_k_means_lloyd.pyx:160\u001b[0m, in \u001b[0;36msklearn.cluster._k_means_lloyd.lloyd_iter_chunked_dense\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_k_means_common.pyx:177\u001b[0m, in \u001b[0;36msklearn.cluster._k_means_common._relocate_empty_clusters_dense\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/core/multiarray.py:346\u001b[0m, in \u001b[0;36mwhere\u001b[0;34m(condition, x, y)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;124;03m    inner(a, b, /)\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    341\u001b[0m \n\u001b[1;32m    342\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, b)\n\u001b[0;32m--> 346\u001b[0m \u001b[38;5;129m@array_function_from_c_func_and_dispatcher\u001b[39m(_multiarray_umath\u001b[38;5;241m.\u001b[39mwhere)\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwhere\u001b[39m(condition, x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    348\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;124;03m    where(condition, [x, y], /)\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;124;03m           [ 0,  3, -1]])\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (condition, x, y)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "dwell_ensemble_svm_cv_multiseed.py  (with AUC-ROC  *and* log-odds / odds-ratios)\n",
    "\n",
    "• Sweeps K_STATES_LIST = [5,6,7,8,9,10]\n",
    "• Runs the best-state pipeline for each k and RANDOM_SEED\n",
    "• Inside every CV fold, aggregates the k-specific predictions by majority vote\n",
    "• Outputs:\n",
    "    ├─ svm_per_seed_k_summary.csv\n",
    "    ├─ svm_per_seed_ensemble_summary.csv\n",
    "    ├─ svm_overall_across_seeds.csv\n",
    "    ├─ svm_mean_confusion_matrix.csv\n",
    "    ├─ logit_coeffs_per_fold.csv          ← UPDATED (includes intercept)\n",
    "    ├─ logit_coeffs_per_seed_k.csv        ← UPDATED (includes intercept_mean/std)\n",
    "    └─ logit_coeffs_overall.csv           ← UPDATED (includes intercept_mean/std)\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix\n",
    ")\n",
    "from sklearn.utils import compute_class_weight\n",
    "from scipy.stats import ttest_ind\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# ─── PARAMETERS ────────────────────────────────────────────────────────────\n",
    "BASE_DIR      = Path(\"/data/s.dharia-ra/PEARL/PEARL/derivatives_selected\")\n",
    "NPZ_DIR       = BASE_DIR / \"subject_windows\"\n",
    "WIN_STEP      = 5\n",
    "TR            = 0.8\n",
    "K_STATES_LIST = list(range(5, 11))\n",
    "N_SPLITS      = 10\n",
    "SEEDS         = [0, 1, 2, 3, 4]\n",
    "\n",
    "# ─── LOAD DATA ONCE ────────────────────────────────────────────────────────\n",
    "subj_ids, labels, windows = [], [], []\n",
    "for npz in sorted(NPZ_DIR.glob(\"*_windows.npz\")):\n",
    "    data = np.load(npz)\n",
    "    lab  = data[\"label\"]\n",
    "    if lab not in (\"A+P-\", \"A+P+\"):\n",
    "        continue\n",
    "    subj_ids.append(npz.stem.split(\"_\")[0])\n",
    "    labels.append(0 if lab == \"A+P-\" else 1)\n",
    "    windows.append(np.vstack([data[\"PA\"], data[\"AP\"]]))\n",
    "labels = np.array(labels)\n",
    "print(f\"Loaded {len(subj_ids)} subjects; label counts = {np.bincount(labels)}\")\n",
    "\n",
    "# ─── HELPERS ───────────────────────────────────────────────────────────────\n",
    "def dwell_times(preds, k):\n",
    "    \"\"\"Convert state-sequence → dwell-time vector (seconds).\"\"\"\n",
    "    return np.bincount(preds, minlength=k) * WIN_STEP * TR\n",
    "\n",
    "def majority_vote(row):\n",
    "    return Counter(row).most_common(1)[0][0]\n",
    "\n",
    "# ─── MAIN LOOP ─────────────────────────────────────────────────────────────\n",
    "rows_k, rows_ens, cms_ens = [], [], []\n",
    "\n",
    "# NEW ─ rows for coefficients (and intercept)\n",
    "rows_coef_fold = []\n",
    "\n",
    "for seed in SEEDS:\n",
    "    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=seed)\n",
    "\n",
    "    # accumulate across folds for ensemble\n",
    "    y_true_all_ens, y_pred_all_ens, y_score_all_ens = [], [], []\n",
    "\n",
    "    # per-seed, per-k metrics\n",
    "    metrics_k = {\n",
    "        k: {m: [] for m in (\"accuracy\",\"precision\",\"recall\",\"f1_macro\",\"aucroc\")}\n",
    "        for k in K_STATES_LIST\n",
    "    }\n",
    "\n",
    "    for fold, (tr_idx, te_idx) in enumerate(skf.split(subj_ids, labels)):\n",
    "        y_pred_mat = np.empty((len(te_idx), len(K_STATES_LIST)), dtype=int)\n",
    "        y_prob_mat = np.zeros((len(te_idx), len(K_STATES_LIST)), dtype=float)\n",
    "\n",
    "        for j, k in enumerate(K_STATES_LIST):\n",
    "            X_tr_win = np.vstack([windows[i] for i in tr_idx])\n",
    "            km = KMeans(n_clusters=k, random_state=seed, n_init=\"auto\")\n",
    "            km.fit(X_tr_win)\n",
    "\n",
    "            dwell_tr = np.vstack([dwell_times(km.predict(windows[i]), k) for i in tr_idx])\n",
    "            dwell_te = np.vstack([dwell_times(km.predict(windows[i]), k) for i in te_idx])\n",
    "            y_tr, y_te = labels[tr_idx], labels[te_idx]\n",
    "\n",
    "            # pick best state via t-test + FDR\n",
    "            pvals = [\n",
    "                ttest_ind(dwell_tr[y_tr==0, s], dwell_tr[y_tr==1, s], equal_var=False).pvalue\n",
    "                for s in range(k)\n",
    "            ]\n",
    "            _, p_fdr, _, _ = multipletests(np.array(pvals), method=\"fdr_bh\")\n",
    "            best_state = int(np.argmin(p_fdr))\n",
    "\n",
    "            X_feat_tr = dwell_tr[:, best_state].reshape(-1, 1)\n",
    "            X_feat_te = dwell_te[:, best_state].reshape(-1, 1)\n",
    "\n",
    "            cw = compute_class_weight(\"balanced\", classes=np.unique(y_tr), y=y_tr)\n",
    "            class_weights = {0: cw[0], 1: cw[1]}\n",
    "\n",
    "            clf = make_pipeline(\n",
    "                MinMaxScaler(),\n",
    "                LogisticRegression(penalty=None, class_weight=class_weights,\n",
    "                                   C=1, tol=1e-3, random_state=seed)\n",
    "            )\n",
    "            clf.fit(X_feat_tr, y_tr)\n",
    "\n",
    "            # ── NEW: extract β, intercept, and odds-ratio ───────────────\n",
    "            logreg   = clf.named_steps[\"logisticregression\"]\n",
    "            beta     = float(logreg.coef_[0, 0])\n",
    "            intercept= float(logreg.intercept_[0])\n",
    "            odds     = np.exp(beta)\n",
    "\n",
    "            rows_coef_fold.append({\n",
    "                \"seed\": seed, \"fold\": fold, \"k\": k,\n",
    "                \"coef\": beta,\n",
    "                \"intercept\": intercept,\n",
    "                \"odds_ratio\": odds\n",
    "            })\n",
    "\n",
    "            # predictions & AUC\n",
    "            y_pred = clf.predict(X_feat_te)\n",
    "            y_prob = clf.predict_proba(X_feat_te)[:, 1]\n",
    "\n",
    "            # store per-k metrics\n",
    "            metrics_k[k][\"accuracy\"].append(accuracy_score(y_te, y_pred))\n",
    "            metrics_k[k][\"precision\"].append(precision_score(y_te, y_pred, average=\"macro\"))\n",
    "            metrics_k[k][\"recall\"].append(recall_score(y_te, y_pred, average=\"macro\"))\n",
    "            metrics_k[k][\"f1_macro\"].append(f1_score(y_te, y_pred, average=\"macro\"))\n",
    "            metrics_k[k][\"aucroc\"].append(roc_auc_score(y_te, y_prob))\n",
    "\n",
    "            y_pred_mat[:, j] = y_pred\n",
    "            y_prob_mat[:, j] = y_prob\n",
    "\n",
    "        # ensemble\n",
    "        y_pred_ens  = np.apply_along_axis(majority_vote, 1, y_pred_mat)\n",
    "        y_score_ens = y_prob_mat.mean(axis=1)\n",
    "        y_true_all_ens.extend(labels[te_idx])\n",
    "        y_pred_all_ens.extend(y_pred_ens)\n",
    "        y_score_all_ens.extend(y_score_ens)\n",
    "\n",
    "    # summarize per-k\n",
    "    for k in K_STATES_LIST:\n",
    "        row = {\"seed\": seed, \"k\": k}\n",
    "        for m in (\"accuracy\",\"precision\",\"recall\",\"f1_macro\",\"aucroc\"):\n",
    "            vals = metrics_k[k][m]\n",
    "            row[f\"{m}_mean\"] = np.mean(vals)\n",
    "            row[f\"{m}_std\"]  = np.std(vals)\n",
    "        rows_k.append(row)\n",
    "\n",
    "    # ensemble summary\n",
    "    cm_ens = confusion_matrix(y_true_all_ens, y_pred_all_ens, labels=[0,1])\n",
    "    cms_ens.append(cm_ens)\n",
    "    rows_ens.append({\n",
    "        \"seed\": seed,\n",
    "        \"accuracy\":  accuracy_score(y_true_all_ens, y_pred_all_ens),\n",
    "        \"precision\": precision_score(y_true_all_ens, y_pred_all_ens, average=\"macro\"),\n",
    "        \"recall\":    recall_score(   y_true_all_ens, y_pred_all_ens, average=\"macro\"),\n",
    "        \"f1_macro\":  f1_score(      y_true_all_ens, y_pred_all_ens, average=\"macro\"),\n",
    "        \"aucroc\":    roc_auc_score( y_true_all_ens, y_score_all_ens)\n",
    "    })\n",
    "\n",
    "# ─── BUILD & SAVE DATAFRAMES ─────────────────────────────────────────────────\n",
    "df_k    = pd.DataFrame(rows_k).set_index([\"seed\",\"k\"])\n",
    "df_ens  = pd.DataFrame(rows_ens).set_index(\"seed\")\n",
    "\n",
    "overall_rows = []\n",
    "for m in (\"accuracy\",\"precision\",\"recall\",\"f1_macro\",\"aucroc\"):\n",
    "    overall_rows.append({\n",
    "        \"metric\": m,\n",
    "        \"mean\":   df_ens[m].mean(),\n",
    "        \"std\":    df_ens[m].std()\n",
    "    })\n",
    "mean_cm = np.mean(cms_ens, axis=0)\n",
    "\n",
    "df_coef_fold   = pd.DataFrame(rows_coef_fold)  # seed × fold × k\n",
    "\n",
    "# per-seed-k summary (incl intercept)\n",
    "df_coef_seed_k = (\n",
    "    df_coef_fold\n",
    "    .groupby([\"seed\",\"k\"])\n",
    "    .agg(\n",
    "        coef_mean        = (\"coef\",        \"mean\"),\n",
    "        coef_std         = (\"coef\",        \"std\"),\n",
    "        intercept_mean   = (\"intercept\",   \"mean\"),\n",
    "        intercept_std    = (\"intercept\",   \"std\"),\n",
    "        odds_ratio_mean  = (\"odds_ratio\",  \"mean\"),\n",
    "        odds_ratio_std   = (\"odds_ratio\",  \"std\")\n",
    "    )\n",
    "    .reset_index()\n",
    "    .set_index([\"seed\",\"k\"])\n",
    ")\n",
    "\n",
    "# overall summary\n",
    "df_coef_overall = pd.DataFrame({\n",
    "    \"coef_mean\"        : [df_coef_fold[\"coef\"].mean()],\n",
    "    \"coef_std\"         : [df_coef_fold[\"coef\"].std()],\n",
    "    \"intercept_mean\"   : [df_coef_fold[\"intercept\"].mean()],\n",
    "    \"intercept_std\"    : [df_coef_fold[\"intercept\"].std()],\n",
    "    \"odds_ratio_mean\"  : [df_coef_fold[\"odds_ratio\"].mean()],\n",
    "    \"odds_ratio_std\"   : [df_coef_fold[\"odds_ratio\"].std()]\n",
    "})\n",
    "\n",
    "# SAVE\n",
    "df_k.to_csv(BASE_DIR / \"svm_per_seed_k_summary.csv\")\n",
    "df_ens.to_csv(BASE_DIR / \"svm_per_seed_ensemble_summary.csv\")\n",
    "pd.DataFrame(overall_rows).to_csv(BASE_DIR / \"svm_overall_across_seeds.csv\", index=False)\n",
    "pd.DataFrame(mean_cm,\n",
    "             index=[\"true_0\",\"true_1\"],\n",
    "             columns=[\"pred_0\",\"pred_1\"]\n",
    "            ).to_csv(BASE_DIR / \"svm_mean_confusion_matrix.csv\")\n",
    "\n",
    "df_coef_fold.to_csv(BASE_DIR / \"logit_coeffs_per_fold.csv\", index=False)\n",
    "df_coef_seed_k.to_csv(BASE_DIR / \"logit_coeffs_per_seed_k.csv\")\n",
    "df_coef_overall.to_csv(BASE_DIR / \"logit_coeffs_overall.csv\", index=False)\n",
    "\n",
    "print(\"\\n✓ Saved metrics, confusion matrices, and updated coefficient tables (with intercept).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01db72b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 42 subjects; label counts = [23 19]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s.dharia-ra/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/s.dharia-ra/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/s.dharia-ra/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/s.dharia-ra/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/s.dharia-ra/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/s.dharia-ra/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/s.dharia-ra/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/s.dharia-ra/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/s.dharia-ra/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/s.dharia-ra/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/s.dharia-ra/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/s.dharia-ra/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/s.dharia-ra/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/s.dharia-ra/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/s.dharia-ra/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/s.dharia-ra/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/s.dharia-ra/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/s.dharia-ra/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/s.dharia-ra/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/s.dharia-ra/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/s.dharia-ra/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/s.dharia-ra/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/s.dharia-ra/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/s.dharia-ra/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/s.dharia-ra/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/s.dharia-ra/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/s.dharia-ra/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/s.dharia-ra/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/s.dharia-ra/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/s.dharia-ra/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/s.dharia-ra/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Saved Random-Forest metrics, confusion matrices, and feature-importance tables.\n"
     ]
    }
   ],
   "source": [
    "# #!/usr/bin/env python3\n",
    "# # -*- coding: utf-8 -*-\n",
    "# \"\"\"\n",
    "# dwell_ensemble_rf_cv_multiseed.py\n",
    "# =================================\n",
    "# • Uses the *full* dwell-time vector (k states) as features\n",
    "# • Classifier: RandomForest\n",
    "# • Sweeps K_STATES_LIST = [5,6,7,8,9,10]\n",
    "# • 10-fold CV × 5 random seeds\n",
    "# • Majority-vote ensemble over k values inside each outer fold\n",
    "\n",
    "# Outputs\n",
    "# -------\n",
    "#     ├─ rf_per_seed_k_summary.csv\n",
    "#     ├─ rf_per_seed_ensemble_summary.csv\n",
    "#     ├─ rf_overall_across_seeds.csv\n",
    "#     ├─ rf_mean_confusion_matrix.csv\n",
    "#     └─ rf_feat_importances_per_seed_k.csv      ← optional\n",
    "# \"\"\"\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from pathlib import Path\n",
    "# from collections import Counter\n",
    "\n",
    "# from sklearn.cluster import KMeans\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "# from sklearn.metrics import (\n",
    "#     accuracy_score, precision_score, recall_score,\n",
    "#     f1_score, roc_auc_score, confusion_matrix\n",
    "# )\n",
    "# from sklearn.utils import compute_class_weight\n",
    "\n",
    "# # ─── PARAMETERS ────────────────────────────────────────────────────────────\n",
    "# BASE_DIR      = Path(\"/data/s.dharia-ra/PEARL/PEARL/derivatives_selected\")\n",
    "# NPZ_DIR       = BASE_DIR / \"subject_windows\"\n",
    "# WIN_STEP      = 5\n",
    "# TR            = 0.8\n",
    "# K_STATES_LIST = list(range(5, 11))\n",
    "# N_SPLITS      = 10\n",
    "# SEEDS         = [0, 1, 2, 3, 4]\n",
    "\n",
    "# # ─── LOAD DATA ONCE ────────────────────────────────────────────────────────\n",
    "# subj_ids, labels, windows = [], [], []\n",
    "# for npz in sorted(NPZ_DIR.glob(\"*_windows.npz\")):\n",
    "#     data = np.load(npz)\n",
    "#     lab  = data[\"label\"]\n",
    "#     if lab not in (\"A+P-\", \"A+P+\"):\n",
    "#         continue\n",
    "#     subj_ids.append(npz.stem.split(\"_\")[0])\n",
    "#     labels.append(0 if lab == \"A+P-\" else 1)\n",
    "#     windows.append(np.vstack([data[\"PA\"], data[\"AP\"]]))\n",
    "# labels = np.array(labels)\n",
    "# print(f\"Loaded {len(subj_ids)} subjects; label counts = {np.bincount(labels)}\")\n",
    "\n",
    "# # ─── HELPERS ───────────────────────────────────────────────────────────────\n",
    "# def dwell_times(preds, k):\n",
    "#     \"\"\"Convert state indices → dwell-time vector (seconds).\"\"\"\n",
    "#     return np.bincount(preds, minlength=k) * WIN_STEP * TR\n",
    "\n",
    "# def majority_vote(row):\n",
    "#     return Counter(row).most_common(1)[0][0]\n",
    "\n",
    "# # ─── MAIN LOOP ─────────────────────────────────────────────────────────────\n",
    "# rows_k, rows_ens, cms_ens = [], [], []\n",
    "# rows_feat_imp = []        # optional\n",
    "\n",
    "# for seed in SEEDS:\n",
    "#     skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=seed)\n",
    "\n",
    "#     # accumulate across folds for ensemble\n",
    "#     y_true_all_ens, y_pred_all_ens, y_score_all_ens = [], [], []\n",
    "\n",
    "#     # per-seed, per-k metrics\n",
    "#     metrics_k = {\n",
    "#         k: {m: [] for m in (\"accuracy\",\"precision\",\"recall\",\"f1_macro\",\"aucroc\")}\n",
    "#         for k in K_STATES_LIST\n",
    "#     }\n",
    "\n",
    "#     for fold, (tr_idx, te_idx) in enumerate(skf.split(subj_ids, labels)):\n",
    "#         y_pred_mat = np.empty((len(te_idx), len(K_STATES_LIST)), dtype=int)\n",
    "#         y_prob_mat = np.zeros((len(te_idx), len(K_STATES_LIST)), dtype=float)\n",
    "\n",
    "#         for j, k in enumerate(K_STATES_LIST):\n",
    "#             # ---- Clustering over *all* training windows ----\n",
    "#             X_tr_win = np.vstack([windows[i] for i in tr_idx])\n",
    "#             km = KMeans(n_clusters=k, random_state=seed, n_init=\"auto\")\n",
    "#             km.fit(X_tr_win)\n",
    "\n",
    "#             # ---- Build dwell-time feature matrices (k dims) ----\n",
    "#             dwell_tr = np.vstack([dwell_times(km.predict(windows[i]), k) for i in tr_idx])\n",
    "#             dwell_te = np.vstack([dwell_times(km.predict(windows[i]), k) for i in te_idx])\n",
    "#             y_tr, y_te = labels[tr_idx], labels[te_idx]\n",
    "\n",
    "#             # ---- Random-Forest classifier ----\n",
    "#             cw = compute_class_weight(\"balanced\", classes=np.unique(y_tr), y=y_tr)\n",
    "#             class_weights = {0: cw[0], 1: cw[1]}\n",
    "\n",
    "#             clf = make_pipeline(\n",
    "#                 MinMaxScaler(),\n",
    "#                 RandomForestClassifier(\n",
    "#                     n_estimators=500,\n",
    "#                     max_depth=None,\n",
    "#                     class_weight=class_weights,\n",
    "#                     n_jobs=-1,\n",
    "#                     random_state=seed\n",
    "#                 )\n",
    "#             )\n",
    "#             clf.fit(dwell_tr, y_tr)\n",
    "\n",
    "#             # ---- Save feature importances (optional) ----\n",
    "#             rf = clf.named_steps[\"randomforestclassifier\"]\n",
    "#             rows_feat_imp.append({\n",
    "#                 \"seed\": seed,\n",
    "#                 \"fold\": fold,\n",
    "#                 \"k\": k,\n",
    "#                 \"feature_importances\": rf.feature_importances_\n",
    "#             })\n",
    "\n",
    "#             # ---- Predictions & probabilities ----\n",
    "#             y_pred = clf.predict(dwell_te)\n",
    "#             y_prob = clf.predict_proba(dwell_te)[:, 1]\n",
    "\n",
    "#             # ---- Store metrics ----\n",
    "#             metrics_k[k][\"accuracy\"].append(accuracy_score(y_te, y_pred))\n",
    "#             metrics_k[k][\"precision\"].append(precision_score(y_te, y_pred, average=\"macro\"))\n",
    "#             metrics_k[k][\"recall\"].append(recall_score(y_te, y_pred, average=\"macro\"))\n",
    "#             metrics_k[k][\"f1_macro\"].append(f1_score(y_te, y_pred, average=\"macro\"))\n",
    "#             metrics_k[k][\"aucroc\"].append(roc_auc_score(y_te, y_prob))\n",
    "\n",
    "#             y_pred_mat[:, j] = y_pred\n",
    "#             y_prob_mat[:, j] = y_prob\n",
    "\n",
    "#         # ---- Ensemble across k by majority vote ----\n",
    "#         y_pred_ens  = np.apply_along_axis(majority_vote, 1, y_pred_mat)\n",
    "#         y_score_ens = y_prob_mat.mean(axis=1)\n",
    "#         y_true_all_ens.extend(labels[te_idx])\n",
    "#         y_pred_all_ens.extend(y_pred_ens)\n",
    "#         y_score_all_ens.extend(y_score_ens)\n",
    "\n",
    "#     # ---- Summarize per-k ----\n",
    "#     for k in K_STATES_LIST:\n",
    "#         row = {\"seed\": seed, \"k\": k}\n",
    "#         for m in (\"accuracy\",\"precision\",\"recall\",\"f1_macro\",\"aucroc\"):\n",
    "#             vals = metrics_k[k][m]\n",
    "#             row[f\"{m}_mean\"] = np.mean(vals)\n",
    "#             row[f\"{m}_std\"]  = np.std(vals)\n",
    "#         rows_k.append(row)\n",
    "\n",
    "#     # ---- Ensemble summary (per seed) ----\n",
    "#     cm_ens = confusion_matrix(y_true_all_ens, y_pred_all_ens, labels=[0,1])\n",
    "#     cms_ens.append(cm_ens)\n",
    "#     rows_ens.append({\n",
    "#         \"seed\": seed,\n",
    "#         \"accuracy\":  accuracy_score(y_true_all_ens, y_pred_all_ens),\n",
    "#         \"precision\": precision_score(y_true_all_ens, y_pred_all_ens, average=\"macro\"),\n",
    "#         \"recall\":    recall_score(   y_true_all_ens, y_pred_all_ens, average=\"macro\"),\n",
    "#         \"f1_macro\":  f1_score(      y_true_all_ens, y_pred_all_ens, average=\"macro\"),\n",
    "#         \"aucroc\":    roc_auc_score( y_true_all_ens, y_score_all_ens)\n",
    "#     })\n",
    "\n",
    "# # ─── BUILD & SAVE DATAFRAMES ───────────────────────────────────────────────\n",
    "# df_k    = pd.DataFrame(rows_k).set_index([\"seed\",\"k\"])\n",
    "# df_ens  = pd.DataFrame(rows_ens).set_index(\"seed\")\n",
    "\n",
    "# overall_rows = []\n",
    "# for m in (\"accuracy\",\"precision\",\"recall\",\"f1_macro\",\"aucroc\"):\n",
    "#     overall_rows.append({\n",
    "#         \"metric\": m,\n",
    "#         \"mean\":   df_ens[m].mean(),\n",
    "#         \"std\":    df_ens[m].std()\n",
    "#     })\n",
    "# mean_cm = np.mean(cms_ens, axis=0)\n",
    "\n",
    "# # ---- Optional: aggregate feature importances ----\n",
    "# fi_records = []\n",
    "# for rec in rows_feat_imp:\n",
    "#     seed = rec[\"seed\"]; k = rec[\"k\"]; vec = rec[\"feature_importances\"]\n",
    "#     fi_records.extend([\n",
    "#         {\"seed\": seed, \"k\": k, \"state_idx\": i, \"importance\": vec[i]}\n",
    "#         for i in range(len(vec))\n",
    "#     ])\n",
    "# df_feat_imp = (\n",
    "#     pd.DataFrame(fi_records)\n",
    "#       .groupby([\"seed\",\"k\",\"state_idx\"])[\"importance\"]\n",
    "#       .agg([\"mean\",\"std\"])\n",
    "#       .reset_index()\n",
    "#       .set_index([\"seed\",\"k\",\"state_idx\"])\n",
    "# )\n",
    "\n",
    "# # ─── SAVE ──────────────────────────────────────────────────────────────────\n",
    "# df_k.to_csv(BASE_DIR / \"rf_per_seed_k_summary.csv\")\n",
    "# df_ens.to_csv(BASE_DIR / \"rf_per_seed_ensemble_summary.csv\")\n",
    "# pd.DataFrame(overall_rows).to_csv(BASE_DIR / \"rf_overall_across_seeds.csv\", index=False)\n",
    "# pd.DataFrame(mean_cm,\n",
    "#              index=[\"true_0\",\"true_1\"],\n",
    "#              columns=[\"pred_0\",\"pred_1\"]\n",
    "#             ).to_csv(BASE_DIR / \"rf_mean_confusion_matrix.csv\")\n",
    "# df_feat_imp.to_csv(BASE_DIR / \"rf_feat_importances_per_seed_k.csv\")\n",
    "\n",
    "# print(\"\\n✓ Saved Random-Forest metrics, confusion matrices, and feature-importance tables.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f64df11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Plots saved next to the CSV files:\n",
      "   • accuracy_vs_k.png\n",
      "   • ensemble_metric_bars.png\n",
      "   • confusion_matrix_heatmap.png\n",
      "   • feature_importance_heatmap_k{5-10}.png\n"
     ]
    }
   ],
   "source": [
    "# #!/usr/bin/env python3\n",
    "# # -*- coding: utf-8 -*-\n",
    "# \"\"\"\n",
    "# rf_results_viz.py\n",
    "# =================\n",
    "# Visualises Random-Forest dwell-time metrics.\n",
    "\n",
    "# Creates and saves:\n",
    "#     1) accuracy_vs_k.png\n",
    "#     2) ensemble_metric_bars.png\n",
    "#     3) confusion_matrix_heatmap.png\n",
    "#     4) feature_importance_heatmap_k{K}.png   (one per k)\n",
    "\n",
    "# Run from the directory that contains the CSVs emitted by\n",
    "# dwell_ensemble_rf_cv_multiseed.py.\n",
    "# \"\"\"\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# from pathlib import Path\n",
    "# import ast      # for literal list → numpy\n",
    "\n",
    "# BASE_DIR = Path(\".\")      # adjust if needed\n",
    "\n",
    "# # ── 1. Accuracy / F1 vs k ──────────────────────────────────────────────────\n",
    "# df_k = (\n",
    "#     pd.read_csv(BASE_DIR / \"rf_per_seed_k_summary.csv\")\n",
    "#       .set_index([\"seed\", \"k\"])\n",
    "#       .reset_index()\n",
    "# )\n",
    "# # aggregate across seeds\n",
    "# stats = (\n",
    "#     df_k.groupby(\"k\")[[\"accuracy_mean\", \"f1_macro_mean\"]]\n",
    "#         .agg([\"mean\", \"std\"])\n",
    "# )\n",
    "# ks  = stats.index.values\n",
    "# acc = stats[\"accuracy_mean\"][\"mean\"].values\n",
    "# acc_err = stats[\"accuracy_mean\"][\"std\"].values\n",
    "# f1   = stats[\"f1_macro_mean\"][\"mean\"].values\n",
    "# f1_err = stats[\"f1_macro_mean\"][\"std\"].values\n",
    "\n",
    "# plt.figure(figsize=(6,4))\n",
    "# plt.errorbar(ks, acc,  yerr=acc_err,  fmt=\"-o\", label=\"Accuracy\")\n",
    "# plt.errorbar(ks, f1,   yerr=f1_err,   fmt=\"-s\", label=\"F1-macro\")\n",
    "# plt.title(\"CV performance vs k (Random-Forest)\")\n",
    "# plt.xlabel(\"k (number of states)\")\n",
    "# plt.ylabel(\"Score\")\n",
    "# plt.xticks(ks)\n",
    "# plt.ylim(0, 1.0)\n",
    "# plt.grid(alpha=.3)\n",
    "# plt.legend()\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(\"accuracy_vs_k.png\", dpi=300)\n",
    "# plt.close()\n",
    "\n",
    "# # ── 2. Ensemble metrics per seed ───────────────────────────────────────────\n",
    "# df_ens = pd.read_csv(BASE_DIR / \"rf_per_seed_ensemble_summary.csv\").set_index(\"seed\")\n",
    "# metrics = [\"accuracy\", \"precision\", \"recall\", \"f1_macro\", \"aucroc\"]\n",
    "# means = df_ens[metrics].mean().values\n",
    "# errs  = df_ens[metrics].std().values\n",
    "\n",
    "# plt.figure(figsize=(6,4))\n",
    "# x = np.arange(len(metrics))\n",
    "# plt.bar(x, means, yerr=errs, alpha=0.8)\n",
    "# plt.xticks(x, metrics, rotation=30, ha=\"right\")\n",
    "# plt.ylabel(\"Score\")\n",
    "# plt.title(\"Seed-level ensemble performance\")\n",
    "# plt.ylim(0, 1.0)\n",
    "# for xi, m, e in zip(x, means, errs):\n",
    "#     plt.text(xi, m + 0.03, f\"{m:.2f}±{e:.2f}\", ha=\"center\")\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(\"ensemble_metric_bars.png\", dpi=300)\n",
    "# plt.close()\n",
    "\n",
    "# # ── 3. Confusion-matrix heatmap (mean over seeds) ─────────────────────────\n",
    "# cm = pd.read_csv(BASE_DIR / \"rf_mean_confusion_matrix.csv\", index_col=0).values\n",
    "# cm_norm = cm / cm.sum(axis=1, keepdims=True)   # row-normalize\n",
    "\n",
    "# plt.figure(figsize=(3.5,3))\n",
    "# plt.imshow(cm_norm, aspect=\"equal\")\n",
    "# plt.title(\"Mean confusion matrix (normalized)\")\n",
    "# plt.xlabel(\"Predicted label\")\n",
    "# plt.ylabel(\"True label\")\n",
    "# plt.colorbar(fraction=0.046, pad=0.04)\n",
    "# for i in range(2):\n",
    "#     for j in range(2):\n",
    "#         plt.text(j, i, f\"{cm[i,j]}\\n({cm_norm[i,j]:.2f})\",\n",
    "#                  va=\"center\", ha=\"center\", fontsize=9, fontweight=\"bold\")\n",
    "# plt.xticks([0,1], [\"0  (A+P-)\", \"1  (A+P+)\"])\n",
    "# plt.yticks([0,1], [\"0  (A+P-)\", \"1  (A+P+)\"])\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(\"confusion_matrix_heatmap.png\", dpi=300)\n",
    "# plt.close()\n",
    "\n",
    "# # ── 4. Feature-importance heatmaps for each k (optional) ───────────────────\n",
    "# df_imp = (\n",
    "#     pd.read_csv(BASE_DIR / \"rf_feat_importances_per_seed_k.csv\")\n",
    "#       .reset_index()        # seed, k, state_idx, mean, std\n",
    "# )\n",
    "# for k, sub in df_imp.groupby(\"k\"):\n",
    "#     pivot = sub.pivot(index=\"state_idx\", columns=\"seed\", values=\"mean\")\n",
    "#     plt.figure(figsize=(4, max(3, k/2)))\n",
    "#     plt.imshow(pivot.values, aspect=\"auto\")\n",
    "#     plt.title(f\"Feature importances, k={k}\")\n",
    "#     plt.xlabel(\"Seed\")\n",
    "#     plt.ylabel(\"State index\")\n",
    "#     plt.colorbar(fraction=0.046, pad=0.04, label=\"Mean importance\")\n",
    "#     plt.yticks(range(k))\n",
    "#     plt.xticks(range(len(pivot.columns)), pivot.columns)\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(f\"feature_importance_heatmap_k{k}.png\", dpi=300)\n",
    "#     plt.close()\n",
    "\n",
    "# print(\"✓ Plots saved next to the CSV files:\")\n",
    "# print(\"   • accuracy_vs_k.png\")\n",
    "# print(\"   • ensemble_metric_bars.png\")\n",
    "# print(\"   • confusion_matrix_heatmap.png\")\n",
    "# print(\"   • feature_importance_heatmap_k{5-10}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "615b5da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Per-seed ensemble metrics (10-fold CV):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>aucroc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.6190</td>\n",
       "      <td>0.6833</td>\n",
       "      <td>0.6430</td>\n",
       "      <td>0.6047</td>\n",
       "      <td>0.6453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.5476</td>\n",
       "      <td>0.5417</td>\n",
       "      <td>0.5412</td>\n",
       "      <td>0.5411</td>\n",
       "      <td>0.6705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.5952</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.6121</td>\n",
       "      <td>0.5894</td>\n",
       "      <td>0.5606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.6429</td>\n",
       "      <td>0.6786</td>\n",
       "      <td>0.6602</td>\n",
       "      <td>0.6377</td>\n",
       "      <td>0.6934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.5476</td>\n",
       "      <td>0.5556</td>\n",
       "      <td>0.5549</td>\n",
       "      <td>0.5474</td>\n",
       "      <td>0.6247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   seed  accuracy  precision  recall  f1_macro  aucroc\n",
       "0     0    0.6190     0.6833  0.6430    0.6047  0.6453\n",
       "1     1    0.5476     0.5417  0.5412    0.5411  0.6705\n",
       "2     2    0.5952     0.6250  0.6121    0.5894  0.5606\n",
       "3     3    0.6429     0.6786  0.6602    0.6377  0.6934\n",
       "4     4    0.5476     0.5556  0.5549    0.5474  0.6247"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Grand mean ± SD across seeds (already in csv):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.5905</td>\n",
       "      <td>0.0426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>precision</td>\n",
       "      <td>0.6168</td>\n",
       "      <td>0.0665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>recall</td>\n",
       "      <td>0.6023</td>\n",
       "      <td>0.0526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f1_macro</td>\n",
       "      <td>0.5841</td>\n",
       "      <td>0.0404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aucroc</td>\n",
       "      <td>0.6389</td>\n",
       "      <td>0.0508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      metric    mean     std\n",
       "0   accuracy  0.5905  0.0426\n",
       "1  precision  0.6168  0.0665\n",
       "2     recall  0.6023  0.0526\n",
       "3   f1_macro  0.5841  0.0404\n",
       "4     aucroc  0.6389  0.0508"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Per-seed × k grid (peek at top 12 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>k</th>\n",
       "      <th>accuracy_mean</th>\n",
       "      <th>accuracy_std</th>\n",
       "      <th>precision_mean</th>\n",
       "      <th>precision_std</th>\n",
       "      <th>recall_mean</th>\n",
       "      <th>recall_std</th>\n",
       "      <th>f1_macro_mean</th>\n",
       "      <th>f1_macro_std</th>\n",
       "      <th>aucroc_mean</th>\n",
       "      <th>aucroc_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.1187</td>\n",
       "      <td>0.3733</td>\n",
       "      <td>0.1986</td>\n",
       "      <td>0.4833</td>\n",
       "      <td>0.1225</td>\n",
       "      <td>0.4005</td>\n",
       "      <td>0.1485</td>\n",
       "      <td>0.4583</td>\n",
       "      <td>0.2642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.2067</td>\n",
       "      <td>0.4817</td>\n",
       "      <td>0.2916</td>\n",
       "      <td>0.5583</td>\n",
       "      <td>0.2077</td>\n",
       "      <td>0.4905</td>\n",
       "      <td>0.2440</td>\n",
       "      <td>0.5875</td>\n",
       "      <td>0.2625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.630</td>\n",
       "      <td>0.2182</td>\n",
       "      <td>0.6533</td>\n",
       "      <td>0.2702</td>\n",
       "      <td>0.6333</td>\n",
       "      <td>0.2242</td>\n",
       "      <td>0.6052</td>\n",
       "      <td>0.2361</td>\n",
       "      <td>0.6750</td>\n",
       "      <td>0.2163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.2293</td>\n",
       "      <td>0.7167</td>\n",
       "      <td>0.2718</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>0.2273</td>\n",
       "      <td>0.6517</td>\n",
       "      <td>0.2517</td>\n",
       "      <td>0.6083</td>\n",
       "      <td>0.2637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.2003</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.2814</td>\n",
       "      <td>0.5583</td>\n",
       "      <td>0.2206</td>\n",
       "      <td>0.5036</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.5792</td>\n",
       "      <td>0.1692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.1955</td>\n",
       "      <td>0.5250</td>\n",
       "      <td>0.2501</td>\n",
       "      <td>0.5667</td>\n",
       "      <td>0.2034</td>\n",
       "      <td>0.5150</td>\n",
       "      <td>0.2181</td>\n",
       "      <td>0.5667</td>\n",
       "      <td>0.1965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.1847</td>\n",
       "      <td>0.4625</td>\n",
       "      <td>0.2517</td>\n",
       "      <td>0.5083</td>\n",
       "      <td>0.1805</td>\n",
       "      <td>0.4512</td>\n",
       "      <td>0.1881</td>\n",
       "      <td>0.5458</td>\n",
       "      <td>0.2418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.2392</td>\n",
       "      <td>0.5250</td>\n",
       "      <td>0.3114</td>\n",
       "      <td>0.5583</td>\n",
       "      <td>0.2358</td>\n",
       "      <td>0.5176</td>\n",
       "      <td>0.2614</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.2882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.635</td>\n",
       "      <td>0.2086</td>\n",
       "      <td>0.6208</td>\n",
       "      <td>0.2651</td>\n",
       "      <td>0.6417</td>\n",
       "      <td>0.2142</td>\n",
       "      <td>0.6062</td>\n",
       "      <td>0.2286</td>\n",
       "      <td>0.7042</td>\n",
       "      <td>0.2390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.4883</td>\n",
       "      <td>0.2821</td>\n",
       "      <td>0.5250</td>\n",
       "      <td>0.2556</td>\n",
       "      <td>0.4908</td>\n",
       "      <td>0.2578</td>\n",
       "      <td>0.7042</td>\n",
       "      <td>0.2544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.2294</td>\n",
       "      <td>0.5383</td>\n",
       "      <td>0.3012</td>\n",
       "      <td>0.5667</td>\n",
       "      <td>0.2438</td>\n",
       "      <td>0.5161</td>\n",
       "      <td>0.2468</td>\n",
       "      <td>0.6208</td>\n",
       "      <td>0.2773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.2294</td>\n",
       "      <td>0.5633</td>\n",
       "      <td>0.2862</td>\n",
       "      <td>0.5667</td>\n",
       "      <td>0.2438</td>\n",
       "      <td>0.5327</td>\n",
       "      <td>0.2394</td>\n",
       "      <td>0.6208</td>\n",
       "      <td>0.2865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    seed   k  accuracy_mean  accuracy_std  precision_mean  precision_std  \\\n",
       "0      0   5          0.480        0.1187          0.3733         0.1986   \n",
       "1      0   6          0.555        0.2067          0.4817         0.2916   \n",
       "2      0   7          0.630        0.2182          0.6533         0.2702   \n",
       "3      0   8          0.680        0.2293          0.7167         0.2718   \n",
       "4      0   9          0.575        0.2003          0.5000         0.2814   \n",
       "5      0  10          0.555        0.1955          0.5250         0.2501   \n",
       "6      1   5          0.530        0.1847          0.4625         0.2517   \n",
       "7      1   6          0.595        0.2392          0.5250         0.3114   \n",
       "8      1   7          0.635        0.2086          0.6208         0.2651   \n",
       "9      1   8          0.530        0.2431          0.4883         0.2821   \n",
       "10     1   9          0.575        0.2294          0.5383         0.3012   \n",
       "11     1  10          0.575        0.2294          0.5633         0.2862   \n",
       "\n",
       "    recall_mean  recall_std  f1_macro_mean  f1_macro_std  aucroc_mean  \\\n",
       "0        0.4833      0.1225         0.4005        0.1485       0.4583   \n",
       "1        0.5583      0.2077         0.4905        0.2440       0.5875   \n",
       "2        0.6333      0.2242         0.6052        0.2361       0.6750   \n",
       "3        0.7000      0.2273         0.6517        0.2517       0.6083   \n",
       "4        0.5583      0.2206         0.5036        0.2419       0.5792   \n",
       "5        0.5667      0.2034         0.5150        0.2181       0.5667   \n",
       "6        0.5083      0.1805         0.4512        0.1881       0.5458   \n",
       "7        0.5583      0.2358         0.5176        0.2614       0.6000   \n",
       "8        0.6417      0.2142         0.6062        0.2286       0.7042   \n",
       "9        0.5250      0.2556         0.4908        0.2578       0.7042   \n",
       "10       0.5667      0.2438         0.5161        0.2468       0.6208   \n",
       "11       0.5667      0.2438         0.5327        0.2394       0.6208   \n",
       "\n",
       "    aucroc_std  \n",
       "0       0.2642  \n",
       "1       0.2625  \n",
       "2       0.2163  \n",
       "3       0.2637  \n",
       "4       0.1692  \n",
       "5       0.1965  \n",
       "6       0.2418  \n",
       "7       0.2882  \n",
       "8       0.2390  \n",
       "9       0.2544  \n",
       "10      0.2773  \n",
       "11      0.2865  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 1. Imports & paths ------------------------------------------------------\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt      # <-- we’ll use plain matplotlib only\n",
    "\n",
    "BASE_DIR = Path(\"/data/s.dharia-ra/PEARL/PEARL/derivatives_selected/results_a+p-\")\n",
    "\n",
    "# --- 2. Load the CSV outputs -------------------------------------------------\n",
    "df_k      = pd.read_csv(BASE_DIR / \"svm_per_seed_k_summary.csv\")          # seed × k\n",
    "df_ens    = pd.read_csv(BASE_DIR / \"svm_per_seed_ensemble_summary.csv\")   # seed\n",
    "overall   = pd.read_csv(BASE_DIR / \"svm_overall_across_seeds.csv\")        # grand mean ± SD\n",
    "mean_cm   = pd.read_csv(BASE_DIR / \"svm_mean_confusion_matrix.csv\", index_col=0)\n",
    "\n",
    "# --- 3. Quick textual summaries ---------------------------------------------\n",
    "print(\"\\nPer-seed ensemble metrics (10-fold CV):\")\n",
    "display(df_ens.round(4))                       # Jupyter nicely formats DataFrames\n",
    "\n",
    "print(\"\\nGrand mean ± SD across seeds (already in csv):\")\n",
    "display(overall.round(4))\n",
    "\n",
    "print(\"\\nPer-seed × k grid (peek at top 12 rows):\")\n",
    "display(df_k.head(12).round(4))\n",
    "\n",
    "# # --- 4. Optional: confusion-matrix heat-map ----------------------------------\n",
    "# fig, ax = plt.subplots()\n",
    "# im = ax.imshow(mean_cm.values, vmin=0, cmap=\"Blues\")          # default colormap\n",
    "# ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"True\")\n",
    "# ax.set_xticks([0,1]); ax.set_xticklabels([\"0 (Control)\",\"1 (A+P+)\"])\n",
    "# ax.set_yticks([0,1]); ax.set_yticklabels([\"0 (Control)\",\"1 (A+P+)\"])\n",
    "# for i in range(mean_cm.shape[0]):\n",
    "#     for j in range(mean_cm.shape[1]):\n",
    "#         ax.text(j, i, f\"{mean_cm.values[i,j]:.1f}\",\n",
    "#                 ha=\"center\", va=\"center\", fontsize=12, color=\"white\")\n",
    "# ax.set_title(\"Mean confusion matrix across seeds (ensemble)\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# --- 4. Confusion-matrix heat-map (scikit-learn version) --------------------\n",
    "from sklearn.metrics import ConfusionMatrixDisplay      # NEW import\n",
    "\n",
    "cm_counts = mean_cm.values                              # 2 × 2 ndarray of counts\n",
    "\n",
    "# --- convert to percentages --------------------------------------------------\n",
    "# cm_percent = (cm_counts / cm_counts.sum()) * 100        # global %\n",
    "cm_percent = (cm_counts / cm_counts.sum(axis=1, keepdims=True)) * 100  # row-wise %\n",
    "\n",
    "# --- plot with sklearn -------------------------------------------------------\n",
    "disp = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=cm_percent,\n",
    "    display_labels=[\"A+P-\", \"A+P+\"],\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "disp.plot(\n",
    "    cmap=\"Blues\",\n",
    "    ax=ax,\n",
    "    colorbar=False,\n",
    "    values_format=\".1f\",        # one decimal place\n",
    ")\n",
    "\n",
    "# --- cosmetics ---------------------------------------------------------------\n",
    "# ax.set_title(\"Mean confusion matrix across seeds (ensemble) — %\", fontsize=16, pad=15)\n",
    "ax.set_xlabel(\"\", fontsize=30, labelpad=10)\n",
    "ax.set_ylabel(\"\", fontsize=30, labelpad=10)\n",
    "ax.tick_params(axis=\"both\", labelsize=18)         # <-- NEW: 14-pt tick labels\n",
    "\n",
    "# make the cell text bigger\n",
    "for txt in ax.texts:\n",
    "    value = float(txt.get_text())          # get current number as float\n",
    "    txt.set_text(f\"{value:.1f}%\")          # re-write with percent sign\n",
    "    txt.set_fontsize(20)                   # keep the larger font\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(BASE_DIR / \"mean_confusion_matrix_percentages_A+P-__vs_A+P+.png\", dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "663b00a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " k   │  coef_mean  ± SD  │  OR_mean  │  [coef 95% CI]      │  p_adj\n",
      "─────┼───────────────────┼───────────┼─────────────────────┼──────────\n",
      "  5   │   -1.129 ± 1.881 │ 0.323 │ [-1.664, -0.595] │ 5.83e-04\n",
      "  6   │   -4.277 ± 2.110 │ 0.014 │ [-4.877, -3.678] │ 2.23e-18\n",
      "  7   │   -4.757 ± 2.049 │ 0.009 │ [-5.339, -4.175] │ 9.03e-21\n",
      "  8   │   -4.944 ± 2.072 │ 0.007 │ [-5.533, -4.355] │ 2.86e-21\n",
      "  9   │   -4.530 ± 2.558 │ 0.011 │ [-5.257, -3.803] │ 4.19e-16\n",
      " 10   │   -4.007 ± 3.012 │ 0.018 │ [-4.863, -3.151] │ 8.83e-12\n",
      "─────┴───────────────────┴───────────┴─────────────────────┴──────────\n",
      "avg   │   -3.941 ± 2.281 │ 0.064 │ [-4.589, -3.293] │ 9.72e-05\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from pathlib import Path\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "def calculate_coeff(coef_vec, alpha=0.05):\n",
    "    coef = np.asarray(coef_vec)\n",
    "    n, df = coef.size, coef.size - 1\n",
    "\n",
    "    mean = coef.mean()\n",
    "    sd   = coef.std(ddof=1)\n",
    "    se   = sd / np.sqrt(n)\n",
    "    t    = mean / se\n",
    "    p    = 2 * stats.t.sf(abs(t), df)\n",
    "\n",
    "    tcrit = stats.t.ppf(1 - alpha/2, df)\n",
    "    ci_lo = mean - tcrit*se\n",
    "    ci_hi = mean + tcrit*se\n",
    "\n",
    "    return mean, sd, t, p, ci_lo, ci_hi\n",
    "\n",
    "# --- load data ---------------------------------------------------------------\n",
    "BASE = Path(\"/data/s.dharia-ra/PEARL/PEARL/derivatives_selected/results_a+p-\")\n",
    "df   = pd.read_csv(BASE / \"logit_coeffs_per_fold.csv\")\n",
    "\n",
    "# --- compute per‐k statistics ------------------------------------------------\n",
    "results = []\n",
    "for k, sub in df.groupby(\"k\"):\n",
    "    mean, sd, t, p_raw, ci_lo, ci_hi = calculate_coeff(sub[\"coef\"].values)\n",
    "    results.append({\n",
    "        \"k\":         k,\n",
    "        \"coef_mean\": mean,\n",
    "        \"coef_sd\":   sd,\n",
    "        \"or_mean\":   np.exp(mean),\n",
    "        \"ci_lo\":     ci_lo,\n",
    "        \"ci_hi\":     ci_hi,\n",
    "        \"p_raw\":     p_raw\n",
    "    })\n",
    "\n",
    "# --- batch‐correct all the p_raw’s ------------------------------------------\n",
    "p_raws = np.array([r[\"p_raw\"] for r in results])\n",
    "_, p_adjeds, _, _ = multipletests(p_raws, alpha=0.05, method=\"bonferroni\")\n",
    "\n",
    "for r, p_adj in zip(results, p_adjeds):\n",
    "    r[\"p_adj\"] = p_adj\n",
    "\n",
    "# --- build the average row --------------------------------------------------\n",
    "# average each numeric column (including p_adj)\n",
    "avg_row = {\n",
    "    \"k\":         \"avg\",\n",
    "    \"coef_mean\": np.mean([r[\"coef_mean\"] for r in results]),\n",
    "    \"coef_sd\":   np.mean([r[\"coef_sd\"]   for r in results]),\n",
    "    \"or_mean\":   np.mean([r[\"or_mean\"]   for r in results]),\n",
    "    \"ci_lo\":     np.mean([r[\"ci_lo\"]     for r in results]),\n",
    "    \"ci_hi\":     np.mean([r[\"ci_hi\"]     for r in results]),\n",
    "    \"p_adj\":     np.mean([r[\"p_adj\"]     for r in results])\n",
    "}\n",
    "\n",
    "# --- print the table --------------------------------------------------------\n",
    "header = \" k   │  coef_mean  ± SD  │  OR_mean  │  [coef 95% CI]      │  p_adj\"\n",
    "sep    = \"─────┼───────────────────┼───────────┼─────────────────────┼──────────\"\n",
    "print(header)\n",
    "print(sep)\n",
    "for r in results:\n",
    "    print(f\"{r['k']:>3d}   │ {r['coef_mean']:+8.3f} ± {r['coef_sd']:.3f} │ \"\n",
    "          f\"{r['or_mean']:.3f} │ \"\n",
    "          f\"[{r['ci_lo']:+.3f}, {r['ci_hi']:+.3f}] │ \"\n",
    "          f\"{r['p_adj']:.2e}\")\n",
    "print(\"─────┴───────────────────┴───────────┴─────────────────────┴──────────\")\n",
    "print(f\"{avg_row['k']:>3s}   │ {avg_row['coef_mean']:+8.3f} ± {avg_row['coef_sd']:.3f} │ \"\n",
    "      f\"{avg_row['or_mean']:.3f} │ \"\n",
    "      f\"[{avg_row['ci_lo']:+.3f}, {avg_row['ci_hi']:+.3f}] │ \"\n",
    "      f\"{avg_row['p_adj']:.2e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6a04c6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== potential outlier folds ===\n",
      " k | fold |   beta   |    OR    |   z-score\n",
      "---------------------------------------------\n",
      " 5 |    5 |   +0.374 |      1.5 |  +1.25\n",
      " 5 |    6 |   +0.656 |      1.9 |  +1.79\n",
      " 5 |    7 |   +0.044 |      1.0 |  +0.61\n",
      " 5 |    9 |   +0.345 |      1.4 |  +1.19\n",
      " 5 |    0 |   +0.329 |      1.4 |  +1.16\n",
      " 5 |    5 |   +0.336 |      1.4 |  +1.17\n",
      " 5 |    6 |   +0.521 |      1.7 |  +1.53\n",
      " 5 |    7 |   +0.200 |      1.2 |  +0.91\n",
      " 5 |    1 |   +0.432 |      1.5 |  +1.36\n",
      " 5 |    2 |   +0.234 |      1.3 |  +0.98\n",
      " 5 |    4 |   +0.421 |      1.5 |  +1.34\n",
      " 5 |    6 |   +0.751 |      2.1 |  +1.98\n",
      " 5 |    9 |   +0.163 |      1.2 |  +0.84\n",
      " 5 |    0 |   +0.190 |      1.2 |  +0.89\n",
      " 5 |    1 |   +0.162 |      1.2 |  +0.84\n",
      " 5 |    4 |   +0.772 |      2.2 |  +2.01\n",
      " 5 |    8 |   +0.244 |      1.3 |  +1.00\n",
      " 6 |    9 |   +0.380 |      1.5 |  +2.90\n",
      " 6 |    2 |   +0.306 |      1.4 |  +2.72\n",
      " 6 |    4 |   +0.421 |      1.5 |  +2.99\n",
      " 7 |    8 |   +0.504 |      1.7 |  +3.60\n",
      " 7 |    8 |   +1.057 |      2.9 |  +4.98\n",
      " 8 |    0 |   +0.483 |      1.6 |  +3.18\n",
      " 8 |    8 |   +0.372 |      1.5 |  +2.94\n",
      " 8 |    5 |   +1.150 |      3.2 |  +4.67\n",
      " 9 |    5 |   +0.499 |      1.6 |  +3.49\n",
      " 9 |    6 |   +1.005 |      2.7 |  +4.72\n",
      "10 |    6 |   +0.718 |      2.1 |  +2.52\n",
      "10 |    4 |   +1.550 |      4.7 |  +3.83\n",
      "10 |    0 |   +1.167 |      3.2 |  +3.22\n",
      "10 |    6 |   +0.981 |      2.7 |  +2.93\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# 0.  decide how you want to call something an “outlier”\n",
    "#     here are two common, easy-to-explain rules\n",
    "# ---------------------------------------------------------------\n",
    "Z_THRESHOLD  = 3        # |z| > 3  (classical)\n",
    "OR_THRESHOLD = 10       # OR larger than this is suspicious\n",
    "\n",
    "# make sure we know which row came from which fold ----------------\n",
    "# (assume the CSV already has a column called \"fold\"; if not,    )\n",
    "# (use reset_index() or enumerate() to create one yourself)      #\n",
    "if \"fold\" not in df.columns:\n",
    "    df = df.reset_index().rename(columns={\"index\": \"fold\"})\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 1.  loop through k and look for outliers\n",
    "# ---------------------------------------------------------------\n",
    "outliers = []          # stash (k, fold, beta, OR, z) tuples here\n",
    "\n",
    "for k, sub in df.groupby(\"k\"):\n",
    "    betas   = sub[\"coef\"].values\n",
    "    or_vals = np.exp(betas)\n",
    "\n",
    "    # z-scores of β within this k block\n",
    "    zscores = stats.zscore(betas, ddof=1)\n",
    "\n",
    "    for row, beta, OR, z in zip(sub.itertuples(), betas, or_vals, zscores):\n",
    "        #   Rule 1 : |z| > 3   OR\n",
    "        #   Rule 2 : OR > 10   OR\n",
    "        #   Rule 3 : β > 0     (unexpected sign)      ← optional\n",
    "        if (abs(z) > Z_THRESHOLD) or (OR > OR_THRESHOLD) or (beta > 0):\n",
    "            outliers.append((k, row.fold, beta, OR, z))\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 2.  display what we found\n",
    "# ---------------------------------------------------------------\n",
    "if outliers:\n",
    "    print(\"=== potential outlier folds ===\")\n",
    "    print(\" k | fold |   beta   |    OR    |   z-score\")\n",
    "    print(\"---------------------------------------------\")\n",
    "    for k, fold, beta, OR, z in outliers:\n",
    "        print(f\"{k:2d} | {fold:4d} | {beta:+8.3f} | {OR:8.1f} | {z:+6.2f}\")\n",
    "else:\n",
    "    print(\"No folds exceeded the chosen outlier thresholds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c784f14d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.018315661217203738"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = 2.718281\n",
    "\n",
    "e**-4.00  # same as above, but more explicit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4c0928",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32e4606b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.625"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/0.064"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ea820c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.13533528,  1.        , 20.08553692])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.exp(-5)    # 0.006737947...\n",
    "np.exp([-2, 0, 3])   # array([0.13533528, 1.        , 20.08553692])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ceb475",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
